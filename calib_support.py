#! /usr/bin/env python
#  -*- coding: utf-8 -*-
#
# Support module generated by PAGE version 6.2
#  in conjunction with Tcl version 8.6
# Point à ameliorer :
#   -    dans data_importation :
#       -   détecter autamatiquement la séquence
#   -   dans modele_generation :
#       -   préciser le ration pour un gabarit dynamique
#    Aug 30, 2021 04:41:38 PM CEST  platform: Windows NT

import sys
import platform
import os
from os.path import join,getsize,exists
from datetime import datetime
import pandas as pd
import numpy as np
import random
import pickle as pkl
import matplotlib.pyplot as plt


from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import RandomForestRegressor
import sklearn.metrics as metrics
import xgboost as xgb
from itertools import combinations

try:
    from Tkinter import *
    import tkMessageBox
    import tkFont
    import tkFileDialog
except ImportError:
    from tkinter import *
    from tkinter import messagebox
    from tkinter import font
    from tkinter import filedialog

try:
    import ttk
    py3 = 0
except ImportError:
    import tkinter.ttk as ttk
    py3 = 1

def set_Tk_var():
    # Imp
    global ImportPath
    ImportPath = StringVar()
    ImportPath.set('C:\\Users\\G604088\\OneDrive - Sagemcom Broadband SAS\\Documents\\2023\\2 - DATABASE\\3 - FARYS_NP_DECO\\MERGED')

    global Temp
    Temp = StringVar()
    Temp.set('10, 20, 30')

    global Seq
    Seq = StringVar()
    Seq.set('4, 6.25, 10, 12.8, 16, 25, 50, 100, 150, 280, 500, 700, 1350, 2000, 2500, 3125, 4000')

    global ModeSeq
    ModeSeq = BooleanVar()
    ModeSeq.set(False)

    global ProductComboValue
    ProductComboValue = StringVar()

    global FiltreDebit
    FiltreDebit = BooleanVar()
    FiltreDebit.set(True)

    global TxtImport
    TxtImport = StringVar()

    # GEN

    global GenPath
    GenPath = StringVar()

    global Mes
    Mes = StringVar()
    Mes.set('16, 50, 280,700, 2500, 3125')

    global TempRef
    TempRef = DoubleVar()
    TempRef.set(20)

    global Ratio
    Ratio = DoubleVar()
    Ratio.set(630)

    global ValRatio
    ValRatio = DoubleVar()
    ValRatio.set(0.1)

    global DN
    DN = StringVar()
    DN.set('DN20')

    global Q1,Q2,Q3
    Q1 = DoubleVar()
    Q2 = DoubleVar()
    Q3 = DoubleVar()

def get_flow_from_file():
    if not os.path.exists('temp'):
        os.makedirs('temp')
    os.chdir('temp')
    #if os.path.exists('flow.txt'):
    #    break
    return 0


def init(top, gui, *args, **kwargs):
    global w, top_level, root
    w = gui
    top_level = top
    root = top

def OnBtnExit(p1):
    print('calib_support.OnBtnExit')
    sys.stdout.flush()
    destroy_window()

def onBtnGen(p1):
    print("GENERATION DU MODELE...")
    sys.stdout.flush()
    calcul_q()
    generateModeleOpt()
    GenPath.set(GenPath.get() + '/Output')

    print("MODELE GENERE")

def calcul_q():
    Q3.set(np.where(DN.get()=="DN15", 2500, 4000).item())
    Q1.set(Q3.get()/Ratio.get())
    Q2.set(Q1.get()*1.6)

def callbackFunc(event):
    print("Nouveau DN selectionne")

def OnBtnImp(p1):
    print('calib_support.OnBtnGo')
    print("CHARGEMENT DES DONNES...")
    sys.stdout.flush()
    data_importation()
    GenPath.set(ImportPath.get()+'/Output')
    print("CHARGEMENT TERMINE")

def OnBtnSearchPath():
    print('calib_support.OnBtnSearchPath')
    sys.stdout.flush()
    if (py3 == 1) or (py3 == True):
        path = filedialog.askdirectory()
    else:
        path = tkFileDialog.askdirectory()
    ImportPath.set(path)

def OnTreeviewClick(p1):
    print('calib_support.OnTreeviewClick')
    sys.stdout.flush()

def destroy_window():
    # Function which closes the window.
    global top_level
    top_level.destroy()
    top_level = None

def str2intArray(txt, sep = ','):
    lst = list(map(float, txt.split(sep)))
    return [int(round(i)) for i in lst]

def filesView(rootIn):

    return

def loadCsv(temp):
    """
    Charge les fichiers CAL
    :return:
    """
    os.chdir(ImportPath.get())
    # Liste des fichiers dans les répertoire
    allFiles = os.listdir(ImportPath.get())
    # Sélection des fichiers CAL (avec la température)
    csvFiles = [doc for doc in allFiles if 'CAL' in doc and '.csv' in doc and any([str(deg)+"deg" in doc for deg in temp])]
    # Lecture et concaténation des fichiers CAL
    dfAll = pd.DataFrame()
    for csvFile in csvFiles:
        df = pd.read_csv(csvFile, sep=';', header=0, skiprows=[1])
        df['filename'] = os.path.basename(csvFile)[-10:-4]
        try :
            # Lecture de la date de création
            df['date'] = datetime.strptime(os.path.basename(csvFile)[-10:-4], '%d%m%y')
        except:
            df['date'] = datetime.strptime("030621", '%d%m%y')
        # Concaténation
        dfAll = dfAll.append(df)

    brd = [pd.DataFrame(dfAll[dfAll.temprg==i].SN.unique(), columns=['SN']) for i in temp]
    brd_to_keep = reduce(lambda left,right: pd.merge(left,right,on='SN'), brd)
    dfAll = dfAll[dfAll.SN.isin(brd_to_keep.SN)]
    rdm_brd = np.random.choice(dfAll[dfAll.temprg==20]["Spool Piece"].unique())

    df = dfAll[(dfAll.temprg==20)&(dfAll["Spool Piece"]==rdm_brd)]

    return dfAll,df,allFiles

def data_importation():
    """
    Génère: referentiel.csv
            tranformetemp.scv
            cal.pkl
    """
    temp = str2intArray(Temp.get())

    # TO DO INTEGRER LA GESTION DES DEBITS
    seqComplete = str2intArray(Seq.get())
    seqCompleteAvecZero = np.concatenate(([0], seqComplete))


    dfAll, df, allFiles = loadCsv(temp)


    # Filtrage des débits et températures
    dfAll = dfAll[dfAll.bflow.round().isin(seqCompleteAvecZero)]
    dfAll = dfAll[dfAll.temprg.round().isin(temp)]
    dfAll.reset_index(inplace = True, drop = True)

    df = df[df.bflow.round().isin(seqCompleteAvecZero)]
    df.reset_index(inplace = True, drop = True)

    # Sauvegarde du dernier fichier dans une autre database pour le transformer plus tard dans un référentiel

    if df.temprg.unique() in temp:
        randomFile = df.copy()

    else:
        randomFile = dfAll[(dfAll["Spool Piece"] == dfAll['Spool Piece'].tail(1).values[0]) & (dfAll["temprg"] == dfAll['temprg'].tail(1).values[0])].copy()

    try:
        os.makedirs('Output')
    except OSError:
        if not os.path.isdir('Output'):
            raise

    # Compteur des observations
    dfAll['counter'] = dfAll.groupby(['Spool Piece', 'temprg', 'flowrg']).cumcount() + 1
    # Filtre sur les variables retenues
    dfAll = dfAll[
        ['Spool Piece', 'counter', 'temprg', 'factor_corr', 'AVR_TOF_SUM', 'temp', 'bflow', 'date', 'flowrg', 'dtofc']]

    # Filtre sur les temperatures a 20 deg avant de creer la table pivot
    df20 = dfAll[dfAll.temprg == 20]

    # Recuperation du facteur et tof sum a 20 uniquement par spool piece, information qu'on va ajouter plus tard
    fact_tof = df20[['Spool Piece', 'factor_corr', 'AVR_TOF_SUM', 'bflow']]
    fact_tof = fact_tof[fact_tof.bflow.round() == 3125]
    fact_tof.drop('bflow', axis=1, inplace=True)
    fact_tof['factor_corr_x_avr_tof'] = fact_tof.factor_corr * fact_tof.AVR_TOF_SUM

    # Creation du pivot autour de la database a 20 deg
    # Tous les dtofc passent aussi en colonne
    df2 = df20.pivot_table(index=['Spool Piece', 'counter'], columns='flowrg', values=['dtofc'])

    # Filtre de la database avec tous les cals, on garde que les colones pertinentes
    data = dfAll[['Spool Piece', 'counter', 'temprg', 'temp', 'bflow', 'flowrg', 'date', 'dtofc']]

    # Merge de la base pivot (a 20) avec la base originale
    data2 = df2.merge(data, on=['Spool Piece', 'counter'])
    df = data2.copy()

    # Actualisation des noms des colonnes de la database
    df.columns = ['Spool Piece', 'counter'] + ["dtofc_" + str(i) for i in seqComplete]+['temprg', 'temp', 'bflow', 'flowrg', 'date', 'dtofc']
    # Jointure avec les informations sur le facteur et le sumtof a 20
    df = df.merge(fact_tof, on='Spool Piece', how="inner")

    # Suppression de la variable counter qu'on utilise plus
    df.drop('counter', axis=1, inplace=True)
    # Sauvegarde de la base intermédiaire
    df.to_csv(r'Output\transformetemp.csv', index=False)

    # Creation d'un fichier avec 3 informations principales:
    #   le bflow (arrondi a 2 decimales)
    #   le refvol - volume d'eau utilisé pour ce flow
    #   le nom de la variable qui correspond au dtofc du debit
    randomFile['bflow'] = randomFile.bflow.round()
    randomFile = randomFile[['bflow', 'refvol']]
    randomFile['nomv'] = ["dtofc_" + str(i) for i in seqComplete]

    # Calcul du temps teorique necessaire pour tester ce volume, en minutes
    randomFile['temps'] = randomFile.refvol / randomFile.bflow * 60
    # Sauvegarde du fichier avec les information sur les flows
    randomFile.to_excel(r"Output\referentiel_duree.xlsx", index=False)

    # Recuperation de 3 fichiers CAL aleatoires, 1 a chaque temperature,
    # sur la base desquels nous allons creer en production les cals de sortie
    allFiles = os.listdir(ImportPath.get())

    # Lecture de ces 3 fichiers
    file_temp = []
    df_temp = []
    for i in np.arange(len(temp)):
        file_temp.append(random.choice([file for file in allFiles if str(str(int(temp[i])) + "deg") in file]))

        df_temp_filtered = pd.read_csv(file_temp[i], sep=";", header=0)
        df_temp_filtered = df_temp_filtered[df_temp_filtered.bflow.round().isin(seqCompleteAvecZero)]
        df_temp_filtered.reset_index(drop=True)
        df_temp.append(df_temp_filtered)

    liste_df = [df_temp[i] for i in np.arange(len(temp))]
    os.chdir(str(ImportPath.get() + '\Output'))
    with open('cals_type.pkl', 'wb') as handle:
        pkl.dump(liste_df, handle, protocol=pkl.HIGHEST_PROTOCOL)

    return 0

def mean_absolute_percentage_error(y_true, y_pred, weights_y):
    #y_true, y_pred = np.array(y_true), np.array(y_pred)
    mape = np.mean(np.abs((np.exp(y_true) - np.exp(y_pred)) / np.exp(y_true)) * 100 * weights_y)
    std_mape = np.std((np.exp(y_true) - np.exp(y_pred)) / np.exp(y_true) * 100 * weights_y)
    return mape, std_mape

def evaluate_performance(y_test_x, y_test_hat, weight_x):
    mae = metrics.mean_absolute_error(y_test_x, y_test_hat)
    mse = metrics.mean_squared_error(y_test_x, y_test_hat)
    rmse = np.sqrt(mse)  # or mse**(0.5)
    mape, std_mape = mean_absolute_percentage_error(y_test_x, y_test_hat, weight_x)
    r2 = metrics.r2_score(y_test_x, y_test_hat)
    print("MAE:", mae)
    print("MSE:", mse)
    print("RMSE:", rmse)
    print("R-Squared:", r2)
    print("MAPE avec grille métrologique, en %", mape)
    return mape, std_mape

def grid_search2(dtrain, dtest, params, param1, param2, range1, range2):
    gridsearch_params = [
        (a, b)
        for a in range1
        for b in range2
    ]
    min_mae = float("Inf")
    best_params = None
    num_boost_round = 999
    for a, b in gridsearch_params:
        print(str("CV with "+param1+"={},"+param2+"={}").format(a, b))

        # Update parameters
        params[param1] = a
        params[param2] = b
        # Run CV
        cv_results = xgb.cv(
            params,
            dtrain,
            num_boost_round=num_boost_round,
            seed=42,
            nfold=5,
            metrics={'rmse'}, #mae
            early_stopping_rounds=10)
        # Update best MAE
        mean_mae = cv_results['test-rmse-mean'].min()
        boost_rounds = cv_results['test-rmse-mean'].argmin()
        print("\tMAE {} for {} rounds".format(mean_mae, boost_rounds))
        if mean_mae < min_mae:
            min_mae = mean_mae
            best_params = (a, b)
        print("Best params: {}, {}, MAE: {}".format(best_params[0], best_params[1], min_mae))
    params[param1] = best_params[0]
    params[param2] = best_params[1]
    return params

def grid_search1(dtrain, dtest, params, param1, range):
    min_mae = float("Inf")
    best_params = None
    num_boost_round = 999
    for a in range:
        print("CV with "+param1+"={}".format(a))
        # Update parameters
        params[param1] = a
        # Run CV
        cv_results = xgb.cv(
            params,dtrain,num_boost_round=num_boost_round,seed=42, nfold=5, metrics=['rmse'],early_stopping_rounds=10
        )

        # Update best score
        mean_mae = cv_results['test-rmse-mean'].min()
        boost_round = cv_results['test-rmse-mean'].argmin()
        print("\tMAE {} for {} rounds\n".format(mean_mae, boost_round))
        if mean_mae < min_mae:
            min_mae = mean_mae
            best_params = a
    params[param1] = best_params
    return params

def tuning(params, dtrain, dtest):
    params['eval_metric'] = "rmse"
    num_boost_round = 999

    # Grille des params
    range1 = range(9,12)
    range2 = range(5,8)
    range3 = np.arange(.7,1,.1)
    range_eta = [.3, .2, .1, .05, .01, .005]

    print('step1')
    params = grid_search2(dtrain, dtest, params, 'max_depth', 'min_child_weight', range1, range2)
    print('step2')
    params = grid_search2(dtrain, dtest, params, 'subsample', 'colsample_bytree', range3, range3)
    print('step3')
    params = grid_search1(dtrain, dtest, params, 'eta', range_eta)

    model = xgb.train(params, dtrain, num_boost_round=num_boost_round,evals=[(dtest, "Test")],early_stopping_rounds=10)
    print("Best MAE: {:.2f} in {} rounds".format(model.best_score, model.best_iteration+1))

    num_boost_round = model.best_iteration + 1
    #mean_absolute_error(best_model.predict(dtest), y_test)
    return [params, num_boost_round]

def preprocessing(combinaison):
    # Lecture des fichiers créés durant l'importation
    ref_flows = pd.read_excel(r"referentiel_duree.xlsx")
    # Lecture de la database en entree (qui est encore brute, avec tous les debits en ligne et leur dtofc en colonne)
    df = pd.read_csv(r"transformetemp.csv")


    # Sauvegarde de la longuer de la combinaison
    kappa = len(combinaison)

    # Mise en forme des strings qui decrivent cette combinaison
    combinaison.sort()

    # Liste des variables/colonnes qu'on doit garder pour le modele
    # Variables non-hydrauliques
    to_keep = ['Spool Piece', 'temprg', 'bflow', 'AVR_TOF_SUM', 'factor_corr', 'factor_corr_x_avr_tof']
    # Variables qui correspondent au dtofc des debits mesures + la variable y - dtofc
    to_keep_20 = to_keep + [column for column in list(df.columns) if
                             column in list(ref_flows[ref_flows.bflow.round().isin(combinaison)].nomv)] + ['dtofc']

    # Définition des varibles à retenir
    to_keep = ['Spool Piece', 'temprg', 'bflow', 'AVR_TOF_SUM', 'factor_corr', 'factor_corr_x_avr_tof']
    to_keep = to_keep + [column for column in list(df.columns) if "dtofc" in column]

    # Tri par spool, temperature et debit
    df.sort_values(['Spool Piece', 'temprg', 'bflow'], inplace=True)

    # Creation des fichiers à 20, 30, 40 et 60 pour l'algorithme

    dict = {}
    for t in [str(j) for j in str2intArray(Temp.get())]:
        if t=='20':
            dict[t] = df.copy()
            dict[t] = dict[t][dict[t].temprg == 20]
        else :
            dict[t] = df[df.temprg == int(t)]
            dict[t] = dict[t][to_keep_20]

    return dict, to_keep_20

def results(X_test, y_test, y_hat, to_keep, temp=20):
    df_results = pd.DataFrame(X_test)
    df_results.columns = to_keep[0:-1]
    df_results['temp'] = temp
    df_results['dtofc'] = np.exp(y_test)
    df_results['dtofc_hat'] = np.exp(y_hat)
    df_results['deviation'] = (df_results.dtofc_hat - df_results.dtofc) / df_results.dtofc * 100
    df_results = df_results.sort_values(['Spool Piece', 'bflow'])
    test = df_results.groupby(['Spool Piece', 'bflow']).deviation.describe(percentiles=[0.5, 0.95]).reset_index()
    test = test.sort_values(['Spool Piece', 'bflow'])
    test['spread'] = test['max'] - test['min']

    return [test, df_results]

def gabarit():
    q1 = Q1.get()
    q2 = Q2.get()
    q3 = Q3.get()
    plt.plot([q1, q2, q2, q3*1.25], [5, 5, 2, 2], 'r', linewidth=3)
    plt.plot([q1, q2, q2, q3*1.25], [-5, -5, -2, -2], 'r',linewidth=3)
    plt.plot([1,10000], [0,0], 'r-.', linewidth=3)
    plt.yticks(np.arange(-6, 6, step=1))
    plt.style.use('seaborn-whitegrid')
    plt.xlabel('Flowrate [L/h]')
    plt.ylabel('Error [%]')

    plt.ylim(-6, 6)
    plt.xlim(q1/2, 7500)

def affichage(df, temp):

    # To do : definir une fonction pour calculer les q

    bflow = df.bflow.round().unique()

    df.bflow = df.bflow.round()

    meant = [df[df.bflow == i].deviation.mean() for i in bflow]
    stdt = [df[df.bflow == i].deviation.std() for i in bflow]

    fig = plt.figure(temp,figsize=[20,7])
    ax = plt.subplot(111)
    gabarit()
    #plt.subplot(211)
    for i in df['Spool Piece'].unique():
        plt.semilogx(df[df['Spool Piece'] == i].bflow,
                     df[df['Spool Piece'] == i].deviation, 'o-',label=str(int(i)), linewidth=3)


    ax.grid(b=True, which='minor', color='#999999', linestyle='-')
    ax.grid(b=True, which='major', color='#666666', linestyle='-')

    box = ax.get_position()
    ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
    ax.legend(loc='center left', prop={'size': 14}, bbox_to_anchor=(1, 0.5))
    plt.title(temp)

    if not os.path.exists('Graphiques'):
        os.makedirs('Graphiques')
    plt.savefig(r'Graphiques\\'+str(temp)+'.png')

def trans_data_array(df, kappa, brd_test):
    test_ratio = ValRatio.get()
    test_len = int(df['Spool Piece'].nunique()*test_ratio)

    # temporaire
    #test_len = 1

    #   Creer la liste des brd a selectionner aleatoirement pour le train et le test
    df_train = df[~df["Spool Piece"].isin(brd_test)]
    df_test  = df[df["Spool Piece"].isin(brd_test)]

    X_train_pre = np.array(df_train.iloc[:,:-1])
    X_test_pre  = np.array(df_test.iloc[:,:-1])
    X_train     = X_train_pre[:, 1:kappa + 6]
    X_test      = X_test_pre[:, 1:kappa + 6]

    y_train     = np.array(np.log(df_train['dtofc']))
    y_test      = np.array(np.log(df_test['dtofc']))

    error_weights = np.where(X_test_pre[:, 2] < 9, 1, 2.5)

    return X_train_pre, X_test_pre, X_train, X_test, y_train, y_test, error_weights

def generateModeleOpt():
    sequence = str2intArray(Seq.get())
    combinaison = str2intArray(Mes.get())
    kappa  = len(combinaison)
    temp = str2intArray(Temp.get())
    tempref = TempRef.get()

    os.chdir(GenPath.get())
    #############################################################################################
    # I - Preprocessing
    #############################################################################################
    dict, to_keep_20 = preprocessing(combinaison)
    
    ##############################################################################################
    # II - PROCESSING
    #############################################################################################
    # Pour Temp != 20°C,  seuls la temperature et le dtofc sont utilisés, le reste reste est récupèré
    # depuis le fichier
    # Application d'un traitement supplementaire pour le 20, car pour les debits mesures nous n'avons pas
    # besoin de predire, donc nous supprimons les lignes avec ces debits

    dict['20'] = dict['20'][~dict['20'].bflow.round().isin(combinaison)]
    dict['20'] = dict['20'][to_keep_20]

    """
    Phase d'entrainement
    """
    param = {
        # Parameters that we are going to tune.
        'max_depth': 6,
        'min_child_weight': 1,
        'eta': .3,
        'subsample': 1,
        'colsample_bytree': 1,
        # Other parameters
        'objective': 'reg:linear',
    }
    xgb_m = {}
    y_hat = {}

    test_ratio = ValRatio.get()
    test_len = int(dict["20"]['Spool Piece'].nunique()*test_ratio)

    #   Creer la liste des brd a selectionner aleatoirement pour le train et le test
    brd_test = np.random.choice(dict["20"]["Spool Piece"].unique(), test_len)
    for t in [str(j) for j in str2intArray(Temp.get())]:
        X_train_pre, X_test_pre, X_train, X_test, y_train, y_test, error_weights = trans_data_array(dict[t], kappa, brd_test)
        #X_train, X_test, y_train, y_test, error_weights = trans_data_array(dict[t], kappa)
        dtrain = xgb.DMatrix(X_train, label=y_train)
        dtest = xgb.DMatrix(X_test, label=y_test)
        params, num_boost = tuning(param, dtrain, dtest)

        xgb_m[t] = xgb.train(
            params,
            dtrain,
            num_boost_round=num_boost,
            evals=[(dtest, "Test")]
        )
        # prediction sur l'echantillon de test
        y_hat[t] = xgb_m[t].predict(dtest)

        if ValRatio.get()==0:
            continue
        test, df_results = results(X_test_pre, y_test, y_hat[t], to_keep_20, int(t))

        #df_results['Spool Piece'] = X_test_pre[:, 0].astype('str')
        affichage(df_results, int(t))

        # Retrain the selected model on all data
        X_train_tot= np.row_stack([X_train, X_test])
        y_train_tot= np.row_stack([pd.DataFrame(y_train).values, pd.DataFrame(y_test).values])
        
        dtrain_tot = xgb.DMatrix(X_train_tot, label=y_train_tot)


        xgb_m[t] = xgb.train(
        params,
        dtrain_tot,
        num_boost_round=num_boost,
        evals=[(dtrain_tot, "Train")]
        )

    # On introduit dans une liste les 3 modeles
    liste_modeles = [xgb_m[i] for i in sorted(xgb_m.keys())]
    liste_y_hat = [y_hat[i] for i in sorted(y_hat.keys())]

    with open( r'pickle.pkl', 'wb') as handle:
        pkl.dump(liste_modeles, handle, protocol=pkl.HIGHEST_PROTOCOL)

    return liste_y_hat



    ############################################################################################

if __name__ == '__main__':
    import calib



    calib.vp_start_gui()
